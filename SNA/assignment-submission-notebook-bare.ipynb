{
 "metadata": {
  "name": "assignment-submission-notebook-bare.ipynb",
  "signature": "sha256:80836664979869346c2b91a490e98e4c345c2e9c7505fc606931595db9314fc9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Student first name:** *Manab*\n",
      "\n",
      "**Student last name:** *Chetia*  \n",
      "\n",
      "**Student ANU UNI ID:** *u5492350*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "COMP4650 Social Media Assignment Submission (8 points)\n",
      "==="
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. Assignment questions related to tutorial 1: Constructing and analyzing a social network\n",
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This initial code is necessary for loading into this notebook's Python kernel the object necessary for this part of the assignment. Run the next cell before starting to work on the assignment questions below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv\n",
      "import urllib2\n",
      "import networkx as nx\n",
      "\n",
      "# opening a local CSV file\n",
      "to_read = file(\"dataset.csv\")  #use this line for a locally downloaded file\n",
      "\n",
      "# or reading it directly from the specified URL\n",
      "#url = 'http://mediamining.univ-lyon2.fr/~andrei/sna-lab-ipython/dataset.csv\n",
      "#to_read = urllib2.urlopen(url)\n",
      "\n",
      "reader = csv.reader(to_read)\n",
      "\n",
      "#  construct the networkx graph\n",
      "G = nx.Graph()\n",
      "\n",
      "for line in reader:\n",
      "    if line[0] not in G:  G.add_node(line[0])\n",
      "    if line[1] not in G:  G.add_node(line[1])\n",
      "    G.add_edge(line[0], line[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 57
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**(1 point) Assignment question #1.1:** Find another way of determining the number of users, by applying a (networkx) graph method."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "print \"The social graph contains {} users.\".format( G.number_of_nodes() )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The social graph contains 217 users.\n"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**(0.5 points) Assignment question #1.2:** What is the minimum number of introductions required for the user `'137056623'` to reach any other user? "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "no_hops = max([len(nx.shortest_path(G, source = '137056623', target = x))-1 for x in G.nodes()])\n",
      "print \"Using {} introductions, '137056623' can reach anyone in the network\".format(no_hops)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Using 10 introductions, '137056623' can reach anyone in the network\n"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**(0.5 points) Assignment question #1.3:** What is the minimum number of introductions required for the any user to reach any other user in the  network?  \n",
      "\n",
      "**HINT:** study the `shortest_path_length` method description."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "no_hops = max([nx.shortest_path_length(G, source = y, target = x)  for y in G.nodes() for x in G.nodes() if x!=y])\n",
      "print \"Minimum number of introductions for any user to reach any other user in the network: {}\".format(no_hops)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Minimum number of introductions for any user to reach any other user in the network: 18\n"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**(0.5 points) Assignment question #1.4:** The _diameter_ $d$ of a graph is the maximum eccentricity of any node, $d = \\max_{v} \\epsilon(v)$. Give two ways to compute the diameter of the social network G (one using the calculated eccentricity values calculated earlier and another one using the dedicated `networkx` function)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "ec = nx.eccentricity(G)\n",
      "print \"Diameter of the social network G: {}\".format( nx.diameter(G)   )\n",
      "print \"Diameter of the social network G: {}\".format( max(ec.values()) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Diameter of the social network G: 18\n",
        "Diameter of the social network G: 18\n"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**(0.5 points) Assignment question #1.5:** Determine ALL most central node(s) and ALL most central edge(s), with respect to the node and edge betweenes centrality. Remember that two or more nodes/edges may have the same centrality score and they ALL need to be determined for this assignment.  \n",
      "**Hint:** `networkx` already contains dedicated function to compute node and edge betweenness centrality scores."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "dic1         =  nx.betweenness_centrality(G)\n",
      "centralNodes =  [node for node,score in dic1.iteritems() if score == max(dic1.values())] \n",
      "print \"Most central node(s): {}\".format( centralNodes )\n",
      "\n",
      "dic2         =  nx.edge_betweenness_centrality(G)\n",
      "centralEdges =  [edge for edge,score in dic2.iteritems() if score == max(dic2.values())] \n",
      "print \"Most central edge(s): {}\".format( centralEdges )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Most central node(s): ['54837666']\n",
        "Most central edge(s): [('2260701138', '54837666')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 62
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "2. Assignment questions related to the tutorial 2: constructing a network from real data\n",
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**(0 points) Preliminary assignment question #2.1:** Starting from [the same bzipped JSON twitter dataset](http://mediamining.univ-lyon2.fr/~andrei/sna-lab-ipython/twitter-dump.json.bz2), construct the social graph based on the reply relation. Analyse (as shown in *Step 3*) which fields you require and give the Python code necessary for constructing the network. \n",
      "\n",
      "**NOTE:** all the subsequent questions of this subsection are to be solved on the graph constructed at this question."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\"# cell for analysis of required fields\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import networkx as nx\n",
      "import bz2, json\n",
      "\n",
      "# open the bzipped dataset\n",
      "reader = bz2.BZ2File(\"./twitter-dump.json.bz2\", mode=\"r\")\n",
      "jobj = json.loads(reader.readline())\n",
      "#  construct the networkx graph\n",
      "DG = nx.DiGraph()\n",
      "\n",
      "# go line by line (ergo, tweet by tweet)\n",
      "for line in reader:\n",
      "    # load the JSON object from the read line\n",
      "    jobj = json.loads(line)\n",
      "\n",
      "    # determine user id\n",
      "    user_id = jobj[u'user'][u'id']\n",
      "    if isinstance(user_id, dict):\n",
      "        user_id = user_id.values()[0]\n",
      "    user_id = int(user_id)\n",
      "    \n",
      "    # determine the reply id\n",
      "    reply_id = jobj[u'in_reply_to_user_id']\n",
      "    if reply_id is not None:            \n",
      "        if isinstance(reply_id, dict):\n",
      "            reply_id = reply_id.values()[0]\n",
      "        reply_id = int(reply_id)\n",
      "        \n",
      "        # add the two nodes in the graph, if not already there\n",
      "        if user_id  not in DG:  DG.add_node(user_id)\n",
      "        if reply_id not in DG:  DG.add_node(reply_id)\n",
      "        DG.add_edge(user_id, reply_id)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**(1 point) Assignment question #2.2:** How many nodes and edges do you have in the resulted reply graph?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "print \"Number of nodes: {}\".format( DG.number_of_nodes() )\n",
      "print \"Number of edges: {}\".format( DG.number_of_edges() )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of nodes: 735\n",
        "Number of edges: 622\n"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**(1 point) Assignment question #2.3:** Using the `networkx` functions, filter the social graph to its *giant connected component*. \n",
      "\n",
      "Calculate for this new graph: i) the number of nodes, ii) the number of edges, iii) the radius and iv) the diameter.  \n",
      "\n",
      "**HINT:** You can either construct a new graph which contains only the nodes and edges in the *giant connected component* or you can remove from your initial graph all the nodes and edges belonging to the other connected components.  \n",
      "\n",
      "**NOTE:** This assignments is to be done on the UNDIRECTED version of the social graph. Considering that *DG* is the constructed directed networkx graph, do:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# create an undirected copy of our graph\n",
      "\n",
      "G    = DG.to_undirected()\n",
      "Gcc  = sorted(nx.connected_component_subgraphs(G), key = len, reverse=True)\n",
      "Gccb = Gcc[0]\n",
      "\n",
      "print \"Number of    GCC nodes  : {}\".format( Gccb.number_of_nodes() )\n",
      "print \"Number of    GCC edges  : {}\".format( Gccb.number_of_edges() )\n",
      "print \"Radius of    GCC        : {}\".format( nx.radius(Gccb) )\n",
      "print \"Diameter of  GCC        : {}\".format( nx.diameter(Gccb) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of    GCC nodes  : 465\n",
        "Number of    GCC edges  : 467\n",
        "Radius of    GCC        : 3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Diameter of  GCC        : 6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**(1 point) Assignment question #2.4:** This assignment question is to calculate iterativelly the basic form of the PageRank on the reply social graph constructed previously. Considering that *DG* is the constructed directed networkx graph constructed at the **Assignment questions #2.1**. We will construct the *PageRank* on the largest [*weakly connected component*](http://en.wikipedia.org/wiki/Connectivity_%28graph_theory%29). This is constructed as shown hereafter.\n",
      "\n",
      "Your job is to compute the PageRank, using the algorithm provided in the tutorial and exemplified in the lecture notes.\n",
      "\n",
      "Use a maximum of $maxiter = 20$ iterations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import operator, heapq\n",
      "from itertools import islice\n",
      "\n",
      "# get the list of weakly connected components, sorted by size\n",
      "omp_List = sorted(nx.weakly_connected_component_subgraphs(DG), key = len, reverse=True)\n",
      "\n",
      "# we want the first component, the biggest\n",
      "WCDG = omp_List[0]\n",
      "\n",
      "# get the number of nodes of the graph\n",
      "n = len( WCDG.nodes() )\n",
      "\n",
      "# initialize the PR at moment 0 using a dictionary of pairs {node : score}\n",
      "PR = {}\n",
      "for node in WCDG.nodes():\n",
      "    PR[node] = 1.0/n\n",
      "\n",
      "no_iter  = 0     # current iteration\n",
      "iter_max = 20    # maximum number of iterations\n",
      "d        = 0.7   # decay factor\n",
      "\n",
      "while no_iter < iter_max: # iterate until maximum iterations\n",
      "    new_PR  = {}\n",
      "    no_iter = no_iter + 1\n",
      "\n",
      "    # calculate the PageRank of each node (`new_PR[node]`), based on the previous values (`PR[other_nodes]`)\n",
      "    for node in WCDG.nodes():\n",
      "        \n",
      "        B_p = WCDG.predecessors(node)\n",
      "        \n",
      "        if not len(B_p):\n",
      "            score        = PR[node]\n",
      "            new_PR[node] = ( (1-d)/n ) + d*score\n",
      "        else:\n",
      "            score = 0.0\n",
      "            for b in  B_p:\n",
      "                #out_edges =  WCDG.out_edges(nbunch=b, data=False)\n",
      "                out_edges =  WCDG.out_edges(b)\n",
      "                N_b       =  len(out_edges)\n",
      "                score    +=  PR[b] * (1.0/N_b)\n",
      "            new_PR[node]  = ( (1-d)/n ) + d*score\n",
      "                \n",
      "    # at the end of iteration, replace old values of PR with the new one\n",
      "    PR = new_PR\n",
      "    \n",
      "# # printing at the screen shows some interesting values\n",
      "#sorted_PR = sorted(PR.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
      "\n",
      "# # and print it out the first 10 elements\n",
      "#dict(islice(sorted_PR, 10))\n",
      "top10 = heapq.nlargest (10, PR, key=PR.get)\n",
      "print \"Top 10 elements:\"\n",
      "for node in top10:\n",
      "    print node"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Top 10 elements:\n",
        "34507480\n",
        "34359954\n",
        "153694176\n",
        "1456140289\n",
        "217733121\n",
        "1059729420\n",
        "307812367\n",
        "1304000534\n",
        "2267162648\n",
        "1587808286\n"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "3. Assignment questions related to the tutorial 3: sentiment analysis\n",
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following code is necessary to define the sentiment scoring function that was constructed in the tutorial. Run this before continuing with the assignments:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.stem.wordnet import WordNetLemmatizer\n",
      "import nltk, codecs\n",
      "\n",
      "lmtzr = WordNetLemmatizer()\n",
      "\n",
      "positive_words = [lmtzr.lemmatize(line.strip()) for line in open('sentiment-lexicon-positive-words.txt')]\n",
      "negative_words = [lmtzr.lemmatize(line.strip()) for line in codecs.open('sentiment-lexicon-negative-words.txt', encoding='utf-8')]\n",
      "\n",
      "# # define the function that computes the sentiment score\n",
      "def get_sentiment_score(text):\n",
      "    tokens = nltk.word_tokenize(text)\n",
      "    tweet = [ lmtzr.lemmatize(x.lower()) for x in tokens if len(x) >= 3 ]\n",
      "\n",
      "    # calculate the sentiment score\n",
      "    score = 0\n",
      "    for word in tweet:\n",
      "        if word in positive_words:\n",
      "            score = score + 1\n",
      "        if word in negative_words:\n",
      "            score = score - 1\n",
      "            \n",
      "    return score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 67
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**(1 point) Assignment question #3.1:** Use the `get_sentiment_score(text)`, the sentiment scoring function defined before, and calculate the sentiment polarity of the tweets in the Twitter JSON dataset used in [tutorial 2](http://nbviewer.ipython.org/url/mediamining.univ-lyon2.fr/~andrei/sna-lab-ipython/tutorial-2-construct-network-real-twitter-dump.ipynb). \n",
      "\n",
      "The dataset is [available to download here](http://mediamining.univ-lyon2.fr/~andrei/sna-lab-ipython/twitter-dump.json.bz2). \n",
      "\n",
      "Print the text of the 10 most positive and the 10 most negative tweets. \n",
      "\n",
      "We consider that a tweet $t_1$ is more positive than another tweet $t_2$ when score of the former is higher than the score of the latter ($score(t_1) > score(t_2)$). \n",
      "\n",
      "Similarly, a tweet $t_1$ is more negative than $t_2$ when $score(t_1) < score(t_2)$.  \n",
      "\n",
      "**HINT:** Load the tweets one by one as seen in tutorial 2 and extract the text, which is found in the field *text* of each tweet."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import heapq, bz2, json, operator\n",
      "\n",
      "reader = bz2.BZ2File(\"./twitter-dump.json.bz2\", mode=\"r\")\n",
      "\n",
      "texts = {}\n",
      "for line in reader:\n",
      "    jobj = json.loads(line)\n",
      "\n",
      "    text = jobj['text']\n",
      "    if text is not None:\n",
      "        texts[text] = get_sentiment_score(text)\n",
      "\n",
      "top10positive = heapq.nlargest (10, texts, key=texts.get)\n",
      "top10negative = heapq.nsmallest(10, texts, key=texts.get)\n",
      "\n",
      "print \"Top 10 positive tweets:\"\n",
      "for tweet in top10positive:\n",
      "    print tweet\n",
      "    print \" \"\n",
      "\n",
      "print\n",
      "print\n",
      "\n",
      "print \"Top 10 negative tweets:\"\n",
      "for tweet in top10negative:\n",
      "    print tweet\n",
      "    print \" \"\n",
      "\n",
      "# # To print tweet and score, uncomment the following    \n",
      "# sorted_tweets_p = sorted(texts.items(),key=operator.itemgetter(1),reverse=True) [0:10]\n",
      "# for tweet in sorted_tweets_p:\n",
      "#      print \"Tweet: \", tweet[0],\"Score:\", tweet[1]\n",
      "        \n",
      "# sorted_tweets_n = sorted(texts.items(),key=operator.itemgetter(1),reverse=False)[0:10]\n",
      "# for tweet in sorted_tweets_n:\n",
      "#      print \"Tweet: \", tweet[0],\"Score:\", tweet[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Top 10 positive tweets:\n",
        "It's perfect, Amazing, fantastic, WOW!\ud83d\udc9cI love u\ud83d\udc95 http://t.co/dKDHDhdOgF\n",
        "Please follow me gorgeous!\ud83d\udc51@ArianaGrande \n",
        "#WatchProblemOnVEVO x56\n",
        " \n",
        "RT @PerrieAndSmile: It's perfect, Amazing, fantastic, WOW!\ud83d\udc9cI love u\ud83d\udc95 http://t.co/dKDHDhdOgF\n",
        "Please follow me gorgeous!\ud83d\udc51@ArianaGrande \n",
        "#Watc\u2026\n",
        " \n",
        "Watch @ArianaGrande and @iggyazalea Problem Official music video here \ud83d\udc69\ud83d\udcad\ud83c\udfac http://t.co/hSpUIH9eK6 love love love love love love love \ud83d\udc96\ud83d\udc96\ud83d\udc96\ud83d\udc96\ud83d\ude02\n",
        " \n",
        "It's perfect, Amazing, fantastic, WOW!\ud83d\udc9cI love u\ud83d\udc95 http://t.co/dKDHDhdOgF\n",
        "Please follow me gorgeous!\ud83d\udc51@ArianaGrande \n",
        "#WatchProblemOnVEVO x88\n",
        " \n",
        "It's perfect, Amazing, fantastic, WOW!\ud83d\udc9cI love u\ud83d\udc95 http://t.co/dKDHDhdOgF\n",
        "Please follow me gorgeous!\ud83d\udc51@ArianaGrande \n",
        "#WatchProblemOnVEVO x87\n",
        " \n",
        "RT @ArianaGrande: glad u love it.....\ud83d\udc69 love u more \u201c@indiggynado: @ArianaGrande it is PERFECT , i love you \ud83d\udc95\n",
        "http://t.co/ji6ETHIFuf #WatchP\u2026\n",
        " \n",
        "OMFG! I LOVE IT SO MUCH! LOVE HER!! I can't say how happy i am!! @ArianaGrande amazing http://t.co/oms6SY7yeE\n",
        " \n",
        "Wow!  @ArianaGrande You did a great job! Im so proud of you loves!  http://t.co/dSkFOaAVat  #WatchProblemOnVEVO\n",
        "Guys you should watch it! x3\n",
        " \n",
        "Wow!  @ArianaGrande You did a great job! Im so proud of you loves!  http://t.co/dSkFOaAVat  #WatchProblemOnVEVO\n",
        "Guys you should watch it! x2\n",
        " \n",
        "Wow!  @ArianaGrande You did a great job! Im so proud of you loves!  http://t.co/dSkFOaAVat  #WatchProblemOnVEVO\n",
        "Guys you should watch it! x6\n",
        " \n",
        "\n",
        "\n",
        "Top 10 negative tweets:\n",
        "@ArianaGrande Problem slays my life. Problem slays your life. Problem over everything. Problem slays. https://t.co/b0jyBCGpXn\n",
        " \n",
        "Ariana Grande - Problem ft. Iggy Azalea: http://t.co/RJioyyg2ZK  Attention! lets try to get problem to break the record #WatchProblemOnVEVO\n",
        " \n",
        "RT @_ilysm_Ari: ARIANATORS RT AND SPREAD THE WORLD\n",
        "Problem broke an iTunes record time to break a vevo record. #WatchProblemOnVEVO https://\u2026\n",
        " \n",
        "TWEET THE FUCKING LINK WITH THE HASHTAG. NO ONES GONNA FUCKING GOOGLE THAT SHIT TO FIND IT. http://t.co/Ktrjp69N7E #WatchProblemOnVEVO\n",
        " \n",
        "RT @zaynmgirl: one less problem without ya I got one less problem without ya I got one less problem without ya https://t.co/0tLoIbAVcc\n",
        " \n",
        "Ariana Grande - Problem ft. Iggy Azalea omg I died when I saw it amazingg!! #problem  http://t.co/4RXDEqqb7w\n",
        " \n",
        "I hate those comments that are negative in Ariana's #problemvideo FUCK OFF!! http://t.co/cl6hTuPRLr\n",
        " \n",
        "@ArianaGrande I'm addicted to the video problem, you can follow me? Pls realize my dream! http://t.co/j4IefKwUmw #WatchProblemOnVEVO 198\n",
        " \n",
        "@ArianaGrande I'm addicted to the video problem, you can follow me? Pls realize my dream! http://t.co/j4IefKwUmw #WatchProblemOnVEVO 197\n",
        " \n",
        "RT @floralbocabows: there's 15.4million of us &amp; we only need 19.3million views to break the record, it's not that hard if everyone tries ht\u2026\n",
        " \n"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**(1 point) Assignment question #3.2:** Based on the scores calculated in **Assignment #3.1**, determine the 3 most positive users. A user $u_1$ is more positive than a user $u_2$ if the dataset contains more positive tweets emitted by $u_1$ than tweets emitted by $u_2$. Formally:\n",
      "\n",
      "$$ positivity(u_1) > positivity(u_2) \\iff \\left| \\left\\{ t \\, \\middle| \\, author(t) = u_1 \\wedge score(t) > 0 \\right\\} \\right| > \\left| \\left\\{ t \\, \\middle| \\, author(t) = u_2 \\wedge score(t) > 0 \\right\\} \\right|$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import heapq, bz2, json, operator\n",
      "\n",
      "reader = bz2.BZ2File(\"./twitter-dump.json.bz2\", mode=\"r\")\n",
      "\n",
      "users = {}\n",
      "\n",
      "for line in reader:\n",
      "    jobj = json.loads(line)\n",
      "    user_id = jobj[u'user'][u'id']\n",
      "    \n",
      "    if isinstance(user_id, dict):\n",
      "        user_id = user_id.values()[0]\n",
      "    user_id=int(user_id)\n",
      "    \n",
      "    text= jobj['text']\n",
      "   \n",
      "    if text is not None and get_sentiment_score(text) > 0:        \n",
      "        if user_id not in users:\n",
      "            users[user_id]  = 1\n",
      "        else:\n",
      "            users[user_id] += 1 \n",
      "\n",
      "top10positive =  heapq.nlargest (3, users, key=users.get)      \n",
      "  \n",
      "print \"Top 3 positive users:\"\n",
      "for user in top10positive:\n",
      "    print user\n",
      "\n",
      "# # To print user and score, uncomment the following\n",
      "# sorted_users=sorted(users.items(),key=operator.itemgetter(1),reverse=True)[0:3]\n",
      "# for su in sorted_users:\n",
      "#     print \"User: \",su[0],\"Score: \",su[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Top 3 positive users:\n",
        "1243188937\n",
        "1601537689\n",
        "2229663685\n"
       ]
      }
     ],
     "prompt_number": 69
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Bonus assignment (1 additional points will be given if you solve this assignment correctly, no penalty if you do not solve it. Note that the total grade of SMA assignments cannot exceed 8 points, therefore the bonus point can only be used to compensate for another question which you did not solve corectly):**  \n",
      "\n",
      "We have discussed earlier that our system is fragile to negations: it will score the expression *not beautiful* as positive because it only detects the word beautiful as positive. \n",
      "\n",
      "More generally, we consider that the token **not** changes the polarity of a given token: **not beautiful** becames negative, while **not bad** becomes positive.  \n",
      "\n",
      "Modify the function `get_sentiment_score(text)` to detect the changes of polarity due to the token **not**."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def get_sentiment_score(text):\n",
      "    tokens = nltk.word_tokenize(text)\n",
      "    tweet = [ lmtzr.lemmatize(x.lower()) for x in tokens if len(x) >= 3 ]\n",
      "    \n",
      "    score = 0\n",
      "    \n",
      "    for word in tweet:\n",
      "        if word in positive_words:\n",
      "            score = score + 1\n",
      "        if word in negative_words:\n",
      "            score = score - 1\n",
      "    \n",
      "    if 'not' in tweet:\n",
      "        score = -score\n",
      "    \n",
      "    return score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 70
    }
   ],
   "metadata": {}
  }
 ]
}