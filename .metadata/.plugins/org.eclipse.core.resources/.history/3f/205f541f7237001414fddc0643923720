package KMeans;

import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Paths;
import java.util.ArrayList;
import java.util.HashSet;
import java.util.List;
import java.util.Set;

import nlp.nicta.filters.StopWordChecker;

public class DocUtils 
{
	public static final String SPLIT_TOKENS = "[!\"#$%&'()*+,./:;<=>?\\[\\]^`{|}~\\s]"; // missing: [_-@]
	
	public static List<String> getTokensList(String file_content, boolean ignore_stop_words)
	{
	
		List<String>    tokensList = new ArrayList<String>(); 
		StopWordChecker _swc       = new StopWordChecker();
		
		String[]        tokens     = file_content.split(SPLIT_TOKENS);
		
		for (String token : tokens) 
		{
			token = token.trim().toLowerCase().replaceAll("ï¿½", "'"); 
			if (token.length() == 0 || (ignore_stop_words  &&  _swc.isStopWord(token)))
				continue;
			tokensList.add(token);
		}
		
		return tokensList;
	}
	
	public static Set<String> getTokensSet(List<String> tokensList)
	{
		Set<String> tokensSet = new HashSet<String>(tokensList);
		return tokensSet;
	}
	
	public static void main(String[] args) throws IOException
	{
		String file = "data/blog_data/file_1.txt";
		byte[] encoded = Files.readAllBytes(Paths.get(file));
		String text = new String(encoded, "UTF8");
		print text
		
	}

}
