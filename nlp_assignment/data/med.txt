Overview of the ShARe/CLEF eHealth Evaluation Lab 2013

Discharge summaries and other free-text reports in healthcare transfer information between working shifts and geographic locations. Patients are likely to have difficulties in understanding their content, because of their medical jargon, non-standard  abbreviations, and ward-specific idioms. This paper reports on an evaluation lab with an aim to support the continuum of care by developing methods and resources that make clinical reports in English easier to understand for patients, and which helps them in finding information related to their condition.  This ShARe/CLEFeHealth2013 lab offered student mentoring and shared tasks: identification and normalisation of disorders (1a and 1b) and  normalisation of abbreviations and acronyms (2) in clinical reports with respect to terminology standards in healthcare as well as information retrieval (3) to address questions patients may have when reading clinical reports.  The focus on patients' information needs as opposed to the specialised information needs of physicians and other healthcare workers was the main feature of the lab distinguishing it  from previous shared tasks. De-identified clinical reports for the three tasks were from US intensive care and originated from the MIMIC II database. Other text documents for Task 3 were from the Internet and originated from the Khresmoi project. Task 1 annotations originated from the ShARe annotations.  For Tasks 2 and 3, new annotations, queries, and relevance assessments were created. 64, 56, and 55 people registered their interest in Tasks 1, 2, and 3, respectively.  34 unique teams (3 members per team on average) participated with 22, 17, 5, and 9 teams in Tasks 1a, 1b, 2 and 3, respectively. The teams were from Australia, China, France, India, Ireland, Republic of Korea, Spain, UK, and USA. Some teams developed and used additional annotations, but this strategy contributed to the system performance only in Task 2. The best systems had the F1 score of 0.75 in Task 1a; Accuracies of 0.59 and 0.72 in Tasks 1b and 2; and Precision at 10 of 0.52 in Task 3. The results demonstrate the substantial community interest and capabilities of these systems in making clinical reports easier to understand for patients. The organisers have made data and tools available for future research and development.

Introduction

Discharge summaries transfer information between working shifts and geographical locations.  They are written or dictated by a physician, nurse, therapist, specialist, or other clinician responsible for patient care to describe the course of treatment, the status at release, and care plans. Their primary purpose is to support the care continuum as a handover note between clinicians, but they also serve legal, financial, and administrative purposes. In several countries these documents are regulated by law. For example, in Sweden, the Patient Data Law 255/2008 and in Finland, the Statute 298/2009 on Patient Documents state that in order to ensure good care, clinical documents must cover all necessary information and adequately detail the patient's conditions, care, and recovery.  This legislation also stipulates that the documents must be explicit, comprehensive, and include only generally well-known, accepted concepts and abbreviations.

However, the law and practice differ substantially. The patient and her next of kin are likely to have difficulties in understanding this simple example sentence from a US discharge: “AP: 72 yo f w/ ESRD on HD, CAD, HTN, asthma p/w significant hyperkalemia & associated arrythmias.” After expanding the abbreviations and acronyms as well as correcting the misspellings, they are much more likely to understand that this sentence belongs to the description of the patient's active problem. It tells that the patient is a 72 year old female with dependence on hemodialysis, coronary heart disease, hypertensive disease, and asthma. Her current medical problem (i.e., presenting problem) is significant hyperkalemia and associated arrhythmias. An improved understanding of related concepts in discharge summaries can be achieved by normalising all health conditions to standardised, computer-processable language. In SNOMED-CT, the CUIs C0003811, C0004096, and C0020461 correspond to synonyms of arrhythmia, asthma, and hyperkalemia, respectively.

The patient's and her next-of-kin's understanding of health conditions can be supported not only by these expansions, corrections, and normalisations, but also by linking the words to a patient-centric search on the Internet. Already without electronic linkage with discharge summaries, nearly 70 per cent of search engine users in the USA in 2012 searched for information about health conditions. In 2007, nearly 47 per cent of Europeans considered the Internet as an important source of health information. The search engine could, for example, link hyperkalemia and its synonyms to definitions in Wikipedia, Consumer Health Vocabulary, and other patient-friendly sources. This would explain the connection between hyperkalemia and arrhythmia: Extreme hyperkalemia (having too much potassium in the blood) is a medical emergency due to the risk of potentially fatal arrhythmias (abnormal heart rhythms). The engine should also assess the reliability of information (e.g., guidelines by healthcare service providers vs. uncurated but insightful experiences on discussion forums). 

This paper presents an overview of the ShARe/CLEFeHealth2013 evaluation lab to address these approaches in making clinical text easier to understand and targeting patients' information needs in search on the Internet. The novel lab aimed to develop processing techniques and data for these approaches and an evaluation setting that includes  statistical metrics of correctness and end-user engagement by asking nurses and laypeople to represent patients' preferences in expansions, normalisations, and search. It offered a mentoring track for graduate students working on related fields and shared tasks on NLP and ML: identification and normalisation of disorders (1a and 1b) normalisation of abbreviations and acronyms (2) in clinical reports with respect to terminology standards in healthcare as well as IR (3) to address questions patients may have when reading clinical reports. This attracted 34 teams to submit 113 systems; demonstrated the capabilities of these systems in contributing to patients' understanding and information needs; and made data, guidelines, and  tools available for future research and development. The lab workshop was in CLEF on 23--26 Sep 2013. 

Background

For over forty years, NLP and other techniques based on computational linguistics and ML have been recognised as ways to automate text analysis in healthcare. PubMedreturns 12,860 references, including pioneering studies and recent reviews . Some techniques have progressed from research to use in practice. As US examples, MedLEE used in the New York Presbyterian Hospital normalises patient records to UMLS and Autocoder at the Mayo Clinic in Rochester assigns diagnosis codes to patient records, reducing workload by 80 per cent. 

However, the development and progress has been substantially hindered, but shared tasks address these barriers. The barriers can be classified as lack of access to shared data for system researh, development and evaluation; insufficient common conventions and standards for data, technologies, and evaluations; the formidability of reproducibility; limited collaboration; and lack of user-centered development and scalability.

The first shared tasks related to clinical NLP were in TREC. The 2000 Filtering Track focused on building user profiles to separate relevant and irrelevant documents. Data contained around 350,000 abstracts from the MEDLINE database over five years, manually created topics, and a topic set based on the standardised MeSH The Genomics Track had in 2003-2007 annual IR tasks on genomics data in biomedical papers and clinical reports. The tasks ranged from ad-hoc IR to classification, passage IR, and entity-based question-answering. The Medical Records Track in 2011 and 2012 aimed to develop an IR technique for finding patient cohorts that are relevant to a given criteria for recruitment as populations in comparative effectiveness studies. Their data consisted of de-identified medical records, queries that resemble eligibility criteria of clinical studies, and associated relevance assessments. 

In 2005, ImageCLEFmed introduced annual tasks on accessing to biomedical images in papers and on the Internet.  In 2005-2013, it targeted language-independent techniques for annotating images with concepts; multi-modal IR combining visual and textual features; and multilingual IR techniques.

In 2006, i2B2 began its tasks on clinical NLP:  text de-identification and identification of smoking status in 2006; recognition of obesity and co-morbidities in 2008; medication information extraction in 2009; concept, assertion, and relation recognition in 2010; co-reference analysis in 2011; and temporal-relation analysis in 2012. Data originated from the USA, were in English, and included approximately 1,500 de-identified discharge summaries with their annotations.

Medical NLP Challenges by the Computational Medicine Center in 2007 and 2011 addressed automated diagnosis coding of  radiology reports and classifying the emotions found in suicide notes. In 2007, 1,954 de-identified radiology reports in English from a US radiology department for children were used. In 2011, over 1,000 suicide notes in English were used.

In 2013, the Health Design Challengechallenged to re-imagine the visuals and layout of health/medical records. The purpose was to make the records more usable by and meaningful to patients, their families, and others who take care of them. The challenge was motivated by the continuum of care but did not address NLP and ML. Over 230 teams submitted their designs. The winning designs were announced in Jan 2013 and are showcased on the Internet. 

In Nov 2012 - Feb 2013, NTCIR ran MedNLP on information extraction from simulated medical reports in Japanese. It had text de-identification, complaint/diagnosis recognition, and open tasks. 

Targeting  atients' information needs through NLP, ML  and IR is important, novel, and difficult. Meeting these needs is critical because of the empowering effects the right information and the negative effects missing or incorrect information may have on health outcomes. The focus on patients' and next-of-kins' information needs as opposed to the specialised information needs of healthcare workers is the main distinguishing feature of the ShARe/CLEFeHealth 2013 evaluation lab compared to previous shared tasks. This is, however, technically more difficult, as they represent a wider and more heterogeneous subject population. The variance in, for example, their health profiles, health knowledge, abilities to interpret health information, computer skills, and search queries is greater.

Discussion

This paper reported on a novel evaluation lab with an aim to support the continuum of care by developing methods and resources that make clinical reports in English easier to understand for patients. This ShARe/CLEFeHealth2013 lab had a mentoring track for graduate students and three shared tasks: identification and normalisation of disorders in clinical reports with respect to terminology standards in healthcare; normalisation of abbreviations and acronyms in clinical reports with respect to terminology standards in healthcare; and IR to address questions patients may have when reading clinical reports. The focus on patients' information needs as opposed to the specialised information needs healthcare workers was the main distinguishing feature of the lab from previous shared tasks on NLP and ML. The lab attracted a substantial amount of interest and demonstrated the capabilities of submitted systems and participating teams in making clinical reports easier to understand for patients. Over 30 teams from America, Asia, Australia, and Europe submitted altogether 113 systems to the shared tasks. The best systems had the F1 score of 0.75 in Task 1a; Accuracies of 0.59 and 0.72 in Tasks 1b and 2; and Precision at 10 of 0.52 in Task 3. 

The significance of the lab was emphasised by the organisers' making the text documents, annotations, queries, mappings between queries and the matching clinical report, the matching result sets, relevance assessments, and evaluation tools available for future research and development. The lab developed new annotated datasets, including English text from clinical reports and the Internet. De-identified clinical reports for Task 1-3 were from US intensive care and Task 3 also used documents from the Internet. Task 1 annotations originated from the ShARe annotations, but for Tasks 2 and 3, new annotations, queries, and relevance assessments were created. Guidelines for human subjects training, ethics clearance, research permission, registration, user access, data/annotation format, tools, and contact people were made available.

These three tasks have all aimed at supporting the patient, potential patient or next of kin to understand and have a better picture of their health condition. By working towards easier-to-understand translations of clinical text, we support the patient empowerment and patients' ability to make informed decisions concerning their own health and care.


